---
title: "Missing Data Imputation"
description: |
  An article about methods used for missing data imputation for the SocioEconomic paper.
author:
  - name: Rani Basna 
    url: https://example.com/norajones
    affiliation: University of Gothenburg
    affiliation_url: https://www.gu.se/en/about/find-staff/ranibasna
    orcid_id: 0000-0001-7510-8460
date: "`r Sys.Date()`"
bibliography: miss.bib
csl: 2d-materials.csl
output:
  distill::distill_article:
    #css: hospital_header.css
    #includes:
    #  in_header: header.html
    css: header_2.css
    self_contained: false
    code_folding: true
    # highlight: haddock
    highlight: rstudio
    # highlight_downlit: true
    toc: true
    toc_depth: 2
    # toc_float: true
preview: testBackground.png
categories:
  - Missing data
  - Airway data
  - directed acyclic graph
---

<!-- <style> -->
<!-- html { -->
<!--   scroll-behavior: smooth; -->
<!-- } -->
<!-- d-article { -->
<!--     contain: none; -->
<!--   } -->
<!-- #TOC { -->
<!--   position: fixed; -->
<!--   z-index: 50; -->
<!--   background: #ebebeb;     /* or   background: white; */ -->
<!--   padding: 10px;           /* optional */ -->
<!--   border-radius: 5px;      /* optional */ -->
<!--   } -->

<!-- /* Hide the ToC when resized to mobile or tablet:  480px, 768px, 900px */ -->
<!-- @media screen and (max-width: 900px) { -->
<!-- #TOC { -->
<!--     position: relative; -->
<!--   } -->
<!-- } -->
<!-- </style> -->


```{r xaringanExtra-clipboard, echo=FALSE, include=FALSE}
xaringanExtra::use_clipboard()
```

```{r xaringan-panelset, echo=FALSE, include=FALSE}
xaringanExtra::use_panelset()
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r libraries}
library(mice)
library(dplyr)
library(naniar)
library(finalfit)
library(haven)
library(miceadds)
library(tibble)
library(VIM)
library(bnlearn)
library(dagitty)
```

```{r functions, include=FALSE}
Prepare_data <- function(Smok_Soci_data){
  # make it a data frame object
  my_f_data <- as.data.frame(Smok_Soci_data)
  # dropping uneccesary variables
  drops <- c("ID","cohort")
  my_f_data <- my_f_data[ , !(names(my_f_data) %in% drops)]
  # dropping categorized variables 
  my_f_data <- my_f_data %>% select(-c(cbmi, agecateg, cduration))
  # drop variables with high correlations
  drops_corr <- c("weight", "quitage")
  my_f_data <- my_f_data[ , !(names(my_f_data) %in% drops_corr)]
  # removing non-related variables
  drops_rel <- c("birthyear","height","age_group")
  my_f_data <- my_f_data[ , !(names(my_f_data) %in% drops_rel)]
  # drop more correlated vars
  #drops_corr_cat_2 <- c("asthma_treatmnt","varq10b","cbc","varq10c")
  drops_corr_cat_2 <- c("asthma_treatmnt","cbc")
  my_f_data <- my_f_data[ , !(names(my_f_data) %in% drops_corr_cat_2)]
  
  # if mice is running then also s_amount
  my_f_data <- my_f_data %>% select(-c(asthma_diagnosed, asthma, s_amount))
  
  # from mice models out variables
  my_f_data <- my_f_data %>% select(-c(e_smoking))
  
  # corr_3 correlated with the outcome c_asthma
  drops_corr_cat_3 <- c("alle_asthma","noalle_asthma","w_asthma")
  my_f_data <- my_f_data[ , !(names(my_f_data) %in% drops_corr_cat_3)]
  # remove any_smp and only_symptoms
  my_f_data <- my_f_data %>% select(-c(any_smp, only_symptoms)) # did help a lot
  # trt_copd
  my_f_data <- my_f_data %>% select(- trt_copd) # did help
  # edu_credits
  my_f_data <- my_f_data %>% select(- edu_credits) # did help
  
  # converting variables to factor and numerical
  num_cols <- colnames(my_f_data %>% select( c(BMI, age,duration,startage)))
  fact_cols <-  setdiff(colnames(my_f_data), num_cols)
  # mutare to factors and num
  my_f_data <- my_f_data %>% mutate_at(num_cols, as.numeric)
  my_f_data <- my_f_data %>% mutate_at(fact_cols, factor)
  
  return(my_f_data)
}

# to conclude  we need to remove the edu_credit, s_amount, her_dis, herditery_puldis, any_symp, only_symp, trt_copd
# we need to recreate the any_smp and only_symp afterwords

# data preprocssing

Preprocess_Data <- function(Ready_Raw_data){
  # check the data typee
  if (!(is.data.frame(Ready_Raw_data))){
    stop("The input  is not a dataframe format")
  }
  # ordinal as orderd variables 
  Ready_Raw_data$jabstatus <- ordered(Ready_Raw_data$jabstatus, levels=1:8)
  Ready_Raw_data$sei_class <- ordered(Ready_Raw_data$sei_class, levels=0:7)
  Ready_Raw_data$smoking_status <- ordered(Ready_Raw_data$smoking_status, levels=0:2)
  
  #data_all$education <- ordered(data_all$education, levels=0:2)
  #Ready_Raw_data$edu_credits <- ordered(Ready_Raw_data$edu_credits, levels=0:6)
  #Ready_Raw_data$s_amount <- ordered(Ready_Raw_data$s_amount,levels=0:3)
  Ready_Raw_data$e_amount <- ordered(Ready_Raw_data$e_amount,levels=1:3)
  
  # more ordinal
  Ready_Raw_data$syk_class <- ordered(Ready_Raw_data$syk_class,levels=0:10)
  Ready_Raw_data$SSY_class <- ordered(Ready_Raw_data$SSY_class,levels=0:10) #(2012 classification)
  
  # remove the 2012 classification
  Ready_Raw_data <- Ready_Raw_data %>% select(- SSY_class)
  
  return(Ready_Raw_data)
}

# sensitivity analysis functions ----

sens.mice <- function(IM, ListMethod = ListMethod, SupPar = SupPar){
  if(length(ListMethod) > length(names(IM$data))){
    stop("You have specified too much new methods to be applied.")
  } 
  if(length(ListMethod) < length(names(IM$data))){
    stop("You have not specified enough new methods to be applied.")
  }
  cpt <- 0
  for(i in 1:length(ListMethod)){
    if(ListMethod[i]=="MyFunc"){
      if(IM$method[i] == "norm" | IM$method[i] == "pmm" | IM$method[i] == "logreg"){
        cpt <- cpt + 1
      }
      if(IM$method[i] == "polyreg"){
        cpt <- cpt + dim(table(IM$data[i]))-1
      }
    }
    if(ListMethod[i]==""){
      cpt <- cpt
    }
  }
  if(length(SupPar) > cpt){
    stop("You have specified too much supplementary parameters to be applied.")
  } 
  if(length(SupPar) < cpt){
    stop("You have not specified enough supplementary parameters to be applied.")
  } 
  for(i in 1:length(ListMethod)){
    if(ListMethod[i] != "MyFunc" & ListMethod[i] != ""){
      stop("Values available for ListMethod are ''MyFunc'' and '' ''.")
    }
    if(ListMethod[i]=="Myfunc" & (IM$method[i] =="norm.nob")){
      stop("norm.nob is not an available method for the function sens.mice.")
    }
    if(ListMethod[i]=="Myfunc" & (IM$method[i] =="mean")){
      stop("mean is not an available method for the function sens.mice.")
    }
    if(ListMethod[i]=="Myfunc" & (IM$method[i] =="2l.norm")){
      stop("2l.norm is not an available method for the function sens.mice.")
    }    
    if(ListMethod[i]=="Myfunc" & (IM$method[i] =="lda")){
      stop("lda is not an available method for the function sens.mice.")
    }
    if(ListMethod[i]=="Myfunc" & (IM$method[i] =="sample")){
      stop("sample is not an available method for the function sens.mice.")
    }  
  } 
  j <- 0
  cpt <- 0
  IMinit <- IM
  MyMethod <- IM$method
  listvar <- names(IM$data)
  SumPr <- matrix(NA, nrow=length(ListMethod[ListMethod=="MyFunc"]), ncol=3)
  for(ii in 1:length(listvar)){
    if(ListMethod[ii]=="MyFunc"){
      j <- j + 1
      cpt <- cpt + 1  
      if(MyMethod[ii]=="pmm" | MyMethod[ii]=="norm" | MyMethod[ii]=="logreg"){
        SumPrtemp <- c(listvar[ii], MyMethod[ii], SupPar[j])
        cat(SumPrtemp)   
      }
      if(MyMethod[ii]=="logreg"){
        if(SupPar[j] < 0){
          stop("Value for odds ratio can't be negative.") 
        }
      }
      if(MyMethod[ii]=="polyreg"){
        tempSupPar <- c(SupPar[j : (j + dim(table(IM$data[ii]))-2)])
        SumPrtemp <- c(listvar[ii], MyMethod[ii], tempSupPar)
        cat(SumPrtemp)
        for(m in 1:length(tempSupPar)){
          if(tempSupPar[m] < 0){
            stop("Value for odds ratio can't be negative.")  
          }
        }
      } 
      IMtemp <- IM
      IMtemp$pad$method <- c(rep("", length(names(IM$data))), rep("dummy", length(IM$method) - length(names(IM$data))))
      IMtemp$pad$method[ii] <- "MyFunc"  
      laps <- lapply(1:IM$m, function(x)mice::complete(IM, x))
      temp <- sapply(laps, function(x)mice.impute.MyFunc(IM$data[,ii], !is.na(IM$data[,ii]), 
                                                         model.matrix(~., data=x[,-which(names(x) == listvar[ii])]), SupPar, MyMethod, ii, j))
      IMinit$imp[[listvar[ii]]] <- temp
      if(MyMethod[ii]=="polyreg"){
        j <- (j + dim(table(IM$data[ii]))-2)
      }
      cat("\n")
      SumPr[cpt, 1] <- SumPrtemp[1]
      SumPr[cpt, 2] <- SumPrtemp[2]
      SumPr[cpt, 3] <- SumPrtemp[3]
      if(length(SumPrtemp) > 3){
        temp <- SumPrtemp[3]
        for(l in 4 : length(SumPrtemp)){
          temp <- paste(temp, SumPrtemp[l], sep=" ; ")  
        }
        SumPr[cpt, 3] <- temp
      }         
    }
    if(ListMethod[ii]==""){
      IMinit$imp[listvar[ii]] <- IMinit$imp[listvar[ii]]
    }
  }
  dimnames(SumPr)[[2]] <- c("Variable", "Method", "SupPar") 
  cat("Summary :")
  cat("\n")
  cat("\n")
  print(SumPr)
  IMfinal <- IMinit
}



mice.impute.MyFunc <- function(y, ry, x, suppar , Mymethod, i, j){
  if(Mymethod[i]=="pmm"){
    x <- cbind(1, as.matrix(x))
    parm <- .norm.draw(y, ry, x)
    parm$beta[1] <- parm$beta[1] + suppar[j]
    yhatobs <- x[ry, ] %*% parm$coef
    yhatmis <- x[!ry, ] %*% parm$beta
    return(apply(as.array(yhatmis), 1, .pmm.match, yhat = yhatobs,
                 y = y[ry]))
  }
  if(Mymethod[i]=="norm"){
    x <- cbind(1, as.matrix(x))
    parm <- .norm.draw(y, ry, x)
    parm$beta[1] <- parm$beta[1] + suppar[j]
    return(x[!ry, ] %*% parm$beta + rnorm(sum(!ry)) * parm$sigma)
  }
  if(Mymethod[i]=="logreg"){
    aug <- augment(y, ry, x)
    x <- as.matrix(aug$x)
    y <- aug$y
    ry <- aug$ry
    w <- aug$w
    suppressWarnings(fit <- glm.fit(x[ry, ], y[ry], family = binomial(link = logit),
                                    weights = w[ry]))
    fit.sum <- summary.glm(fit)
    beta <- coef(fit)
    beta[1] <- beta[1] + log(suppar[j])
    rv <- t(chol(fit.sum$cov.unscaled))
    beta.star <- beta + rv %*% rnorm(ncol(rv))
    p <- 1/(1 + exp(-(x[!ry, ] %*% beta.star)))
    vec <- (runif(nrow(p)) <= p)
    vec[vec] <- 1
    if (is.factor(y)) {
      vec <- factor(vec, c(0, 1), levels(y))
    }
    return(vec)
  }
  if(Mymethod[i]=="polyreg"){
    x <- as.matrix(x)
    aug <- augment(y, ry, x)
    x <- aug$x
    y <- aug$y
    ry <- aug$ry
    w <- aug$w   
    ## check whether this works instead of the assign
    tmpData <- cbind.data.frame(y, x)
    fit <- nnet::multinom(formula(tmpData), data = tmpData[ry, ], weights = w[ry],
                          maxit = 200, trace = FALSE) 
    temp <- matrix(fit$wts, nrow=nlevels(y), byrow=T)  
    for(k in 2:nlevels(y)){
      temp[k, 2] <- temp[k, 2] + log(suppar[j])
      j <- j + 1
    }       
    temp <- t(temp)
    fit$wts <- c(temp[, ])
    post <- predict(fit, tmpData[!ry, ], type = "probs")
    if (sum(!ry) == 1)
      post <- matrix(post, nrow = 1, ncol = length(post))
    fy <- as.factor(y)
    nc <- length(levels(fy))
    un <- rep(runif(sum(!ry)), each = nc)
    if (is.vector(post))
      post <- matrix(c(1 - post, post), ncol = 2)
    draws <- un > apply(post, 1, cumsum)
    idx <- 1 + apply(draws, 2, sum)
    return(levels(fy)[idx])     
  }
}


sens.est <- function(mids.obj, vars_vals, digits=2){
  if(!all(names(vars_vals) %in% names(mids.obj$data))){
    stop("Some variables not in imputed data set")
  }
  if(any(sapply(vars_vals, is.matrix))){
    nums <- lapply(1:length(vars_vals), function(x)1:length(vars_vals))
    eg.nums <- do.call("expand.grid", nums)
    eg <-  cn <- NULL
    for(i in 1:ncol(eg.nums)){
      if(is.matrix(vars_vals[[i]])){
        eg <- cbind(eg, vars_vals[[i]][eg.nums[,i], , drop=F])
        cn <- c(cn, paste(names(vars_vals)[i], 1:ncol(vars_vals[[i]]), sep=""))
      }
      else{
        eg <- cbind(eg, vars_vals[[i]][eg.nums[,i]])
        cn <- c(cn, names(vars_vals)[i])
      }
    }
    colnames(eg) <- cn
  }
  else{
    eg <- do.call(expand.grid, vars_vals)[,,drop=FALSE]
    if(!is.matrix(eg)){
      eg <- matrix(eg[[1]], ncol=1)[,,drop=F]
    }    
    colnames(eg) <- names(vars_vals)
  }
  out <- list()
  for(i in 1:nrow(eg)){                                             
    ListMethod <- ifelse(names(mids.obj$data) %in% names(vars_vals), "MyFunc", "")
    SupPar <- c(unlist(eg[i, ,drop=F]))
    out[[i]] <- sens.mice(mids.obj, ListMethod, SupPar)
  }
  nms <- NULL
  for(i in 1:ncol(eg)){
    nms <- cbind(nms, paste(colnames(eg)[i], round(eg[,i], digits), sep=": "))
  }
  names(out) <- apply(nms, 1, paste, collapse=", ")
  out
}

sens.pool <- function(obj, sensData, impData, ...){
  nconds <- length(sensData)
  condlist <- list()
  j <- 1
  for(l in 1:nconds){
    condlist[[j]] <- list()
    for(i in 1:sensData[[j]]$m){
      condlist[[j]][[i]] <- complete(sensData[[j]], i)
    }
    j <- j+1
  }
  condlist[[j]] <- list()
  for(i in 1:impData$m){
    condlist[[(j)]][[i]] <- complete(impData, i)
  }
  { if(length(names(sensData)) > 0){
    names(condlist) <- c(names(sensData), "mice")
  }                                                
    else{
      names(condlist) <- c(as.character(1:nconds), "mice")
    }}
  cond.mods <- list()
  for(i in 1:length(condlist)){
    cond.mods[[i]] <- list()
    for(j in 1:length(condlist[[i]])){
      tmp <- obj
      attr(tmp$terms, ".Environment") <- environment()
      cond.mods[[i]][[j]] <- update(tmp, . ~ ., data=condlist[[i]][[j]])
    }
  }
  comb.mods <- invisible(lapply(cond.mods, MIcombine))
  sum.mods <- invisible(lapply(comb.mods, summary))
  names(comb.mods) <- names(sum.mods) <- names(condlist) 
  sub <- as.data.frame(rbind(do.call(rbind, sum.mods)))
  varnames <- gsub("mice.", "", grep("^mice", rownames(sub), value=T) , fixed=T)
  sub$vars <- as.factor(rep(varnames, length(condlist)))
  sub$conds <- factor(c(rep(names(condlist), each = length(varnames))), levels=names(condlist))
  rownames(sub) <- NULL
  class(sub) <- c("sens.pool", "data.frame")
  sub
}         

plot.sens.pool <- function(x, ...){
  p <- xyplot(results ~ conds | vars , 
              data=x, scales=list(x=list(rot=45), y=list(relation="free", rot=90)), pch=16, col="black", 
              lower=x[["(lower"]], upper=x[["upper)"]], 
              xlab = "", ylab = "Coefficients with 95% Confidence Intervals",
              prepanel=function (x, y, subscripts, lower, upper,...){
                list(ylim = range(c(lower[subscripts], upper[subscripts]), finite = TRUE))},
              panel=function(x,y,lower,upper,subscripts,...){
                panel.xyplot(x, y, ...)
                panel.segments(x, lower[subscripts], x, upper[subscripts], ...)  
                panel.abline(h=0, lty=3)
              })
  p
}                                             

sens.test <- function(obj, var, sensData, impData, digits=3, ...){
  nconds <- length(sensData)
  condlist <- list()
  j <- 1
  for(l in 1:nconds){
    condlist[[j]] <- list()
    for(i in 1:sensData[[j]]$m){
      condlist[[j]][[i]] <- complete(sensData[[j]], i)
    }
    j <- j+1
  }
  condlist[[j]] <- list()
  for(i in 1:impData$m){
    condlist[[(j)]][[i]] <- complete(impData, i)
  }
  if(length(names(sensData)) > 0){
    names(condlist) <- c(names(sensData), "mice")
  }                                                
  else{
    names(condlist) <- c(as.character(1:nconds), "mice")
  }
  cond.mods <- list()
  for(i in 1:length(condlist)){
    cond.mods[[i]] <- list()
    for(j in 1:length(condlist[[i]])){
      tmp <- obj
      attr(tmp$terms, ".Environment") <- environment()
      cond.mods[[i]][[j]] <- update(tmp, . ~ ., data=condlist[[i]][[j]])
    }
  }
  restr.mods <- list()
  for(i in 1:length(condlist)){
    restr.mods[[i]] <- list()
    for(j in 1:length(condlist[[i]])){
      tmp <- obj
      attr(tmp$terms, ".Environment") <- environment()
      restr.mods[[i]][[j]] <- update(tmp, paste0(". ~ .-", var), data=condlist[[i]][[j]])
    }
  }
  full.devs <- lapply(cond.mods, function(x)sapply(x, deviance))
  restr.devs <- lapply(restr.mods, function(x)sapply(x, deviance))
  names(full.devs) <- names(restr.devs) <- names(condlist)
  df.diff <- df.residual(restr.mods[[1]][[1]]) - df.residual(cond.mods[[1]][[1]])
  chisqs <- lapply(1:length(full.devs), function(i)restr.devs[[i]] - full.devs[[i]])
  out <- sapply(1:length(full.devs), function(i)mean(1-pchisq(chisqs[[i]], df.diff)))
  fmt <- paste0("%.", digits, "f")
  out <- cbind(sprintf(fmt, sapply(chisqs, mean)), sprintf(fmt, out))
  colnames(out) <- c("Average X2", "p-value")
  rownames(out) <- names(full.devs)
  cat("Test for exclusion of ", var, "(", df.diff, " degrees of freedom)\n")
  print(noquote(out))
}


augment <- function (y, ry, x, maxcat = 50, ...) {
  # augment comes from mice v. 2.25.  It was not exported from
  # the namespace so could not be imported from that package here
  # I copied the function in its entirety in the interest of continued compatability
  icod <- sort(unique(unclass(y)))
  k <- length(icod)
  if (k > maxcat) 
    stop(paste("Maximum number of categories (", maxcat, 
               ") exceeded", sep = ""))
  p <- ncol(x)
  if (p == 0) 
    return(list(y = y, ry = ry, x = x, w = rep(1, length(y))))
  if (sum(!ry) == 1) 
    return(list(y = y, ry = ry, x = x, w = rep(1, length(y))))
  mean <- apply(x, 2, mean)
  sd <- sqrt(apply(x, 2, var))
  minx <- apply(x, 2, min)
  maxx <- apply(x, 2, max)
  nr <- 2 * p * k
  a <- matrix(mean, nrow = nr, ncol = p, byrow = TRUE)
  b <- matrix(rep(c(rep(c(0.5, -0.5), k), rep(0, nr)), length = nr * 
                    p), nrow = nr, ncol = p, byrow = FALSE)
  c <- matrix(sd, nrow = nr, ncol = p, byrow = TRUE)
  d <- a + b * c
  d <- pmax(matrix(minx, nrow = nr, ncol = p, byrow = TRUE), 
            d)
  d <- pmin(matrix(maxx, nrow = nr, ncol = p, byrow = TRUE), 
            d)
  e <- rep(rep(icod, each = 2), p)
  dimnames(d) <- list(paste("AUG", 1:nrow(d), sep = ""), dimnames(x)[[2]])
  xa <- rbind.data.frame(x, d)
  if (is.factor(y)) 
    ya <- as.factor(levels(y)[c(y, e)])
  else ya <- c(y, e)
  rya <- c(ry, rep(TRUE, nr))
  wa <- c(rep(1, length(y)), rep((p + 1)/nr, nr))
  return(list(y = ya, ry = rya, x = xa, w = wa))
}



sens.wald <- function(obj, hyps, sensData, impData, digits=3, ...){
  nconds <- length(sensData)
  condlist <- list()
  j <- 1
  for(l in 1:nconds){
    condlist[[j]] <- list()
    for(i in 1:sensData[[j]]$m){
      condlist[[j]][[i]] <- complete(sensData[[j]], i)
    }
    j <- j+1
  }
  condlist[[j]] <- list()
  for(i in 1:impData$m){
    condlist[[(j)]][[i]] <- complete(impData, i)
  }
  if(length(names(sensData)) > 0){
    names(condlist) <- c(names(sensData), "mice")
  }                                                
  else{
    names(condlist) <- c(as.character(1:nconds), "mice")
  }
  cond.mods <- list()
  for(i in 1:length(condlist)){
    cond.mods[[i]] <- list()
    for(j in 1:length(condlist[[i]])){
      tmp <- obj
      attr(tmp$terms, ".Environment") <- environment()
      cond.mods[[i]][[j]] <- update(tmp, . ~ ., data=condlist[[i]][[j]])
    }
  }
  comb.mods <- lapply(cond.mods, MIcombine)
  res <- list()
  for(i in 1:length(comb.mods)){
    res[[i]] <- linearHypothesis.default(comb.mods[[1]], hyps, coef.=coef(comb.mods[[i]], vcov. = vcov(comb.mods[[i]])))
    
  }
  out <- sapply(res, function(x)x[2,])
  fmt <- paste0("%.", digits, "f")
  out2 <- t(array(sprintf(fmt, out), dim=dim(out)))
  colnames(out2) <- rownames(out)  
  rownames(out2) <- names(condlist)
  noquote(out2)
}

# aggregate multiple imputations ----
agglomerate.data<-function(data,imp,Mimp,Method="mice"){
  
  Moy<-Mimp+1
  redata<-as.matrix(data)
  ximp<-array(redata,dim=c(nrow(redata),ncol(redata),Moy))
  ####
  
  if(any(is.na(redata))==TRUE){
    if(Method=="mice" || Method=="amelia" || Method=="missmda" || Method=="hmisc" || Method=="norm"){
      #####################MICE
      if(Method=="mice"){
        
        for(i in 1:Mimp){
          ximp[,,i]<-as.matrix(complete(imp,i))
          
        }
        ##Averaged dataset
        ximp[,,Moy]<-apply(ximp[,,1:Mimp],c(1,2),mean)
      }
      #
      #####################Amelia
      if(Method=="amelia"){
        for(i in 1:Mimp){
          ximp[,,i]<-as.matrix(imp$imputations[[i]])
          
        }
        ##Averaged dataset
        ximp[,,Moy]<-apply(ximp[,,1:Mimp],c(1,2),mean)
      }
      #
      ##
      #####################NORM
      if(Method=="norm"){
        for(i in 1:Mimp){
          ximp[,,i]<-as.matrix(imp[[i]])
          
        }
        ##Averaged dataset
        ximp[,,Moy]<-apply(ximp[,,1:Mimp],c(1,2),mean)
      }
      #
      #####################MDA
      if(Method=="missmda"){
        
        for(i in 1:Mimp){
          ximp[,,i]<-as.matrix(imp$res.MI[,,i])
          
        }
        ##Averaged dataset
        ximp[,,Moy]<-apply(ximp[,,1:Mimp],c(1,2),mean)
      }
      #
      ####################Hmisc
      if(Method=="hmisc"){
        ##Extract the m data imputed for each variables
        ximp<-array(redata, dim=c(nrow(redata),ncol(redata),Moy))
        col<-1:ncol(redata)
        for(j in 1:ncol(redata)){
          if(sum(is.na(redata[,j]))==0){
            col<-col[-which(col==j)]
            next
          }
        }
        for(m in 1:Mimp){
          for(g in col){
            ximp[,,m][!complete.cases(ximp[,,m][,g]),g]<-imp$imputed[[g]][,m]
          }
        }
        ##Averaged dataset
        ximp[,,Moy]<-apply(ximp[,,1:Mimp],c(1,2),mean)
        
      }
    }else{
      ##
      ##Warning messages
      cat("Error! You must indicate if you are using Mice, Amelia, missMDA, NORM, or Hmisc package","\n")
    }}else{
      ## Warning messages
      cat("There is no missing value in your dataset","\n")
    }
  #return(ximp)
  tabM<-ximp[,,Moy]
  colnames(tabM)<-colnames(redata)
  list("ImpM"=tabM,"Mi"=ximp[,,1:Mimp],"nbMI"=Mimp, "missing"=as.data.frame(redata))
}#End
###
##
##
##
##Function to draw confidence ellipses
ELLI<-function(x,y,conf=0.95,np)
{centroid<-apply(cbind(x,y),2,mean)
ang <- seq(0,2*pi,length=np)
z<-cbind(cos(ang),sin(ang))
radiuscoef<-qnorm((1-conf)/2, lower.tail=F)
vcvxy<-var(cbind(x,y))
r<-cor(x,y)
M1<-matrix(c(1,1,-1,1),2,2)
M2<-matrix(c(var(x), var(y)),2,2)
M3<-matrix(c(1+r, 1-r),2,2, byrow=T)
ellpar<-M1*sqrt(M2*M3/2)
t(centroid + radiuscoef * ellpar %*% t(z))}
##
##
##
##

plot.MI<-function(IM,symmetric=FALSE,DIM=c(1,2),scale=FALSE,web=FALSE,ellipses=TRUE,...){
  if(any(is.na(IM$ImpM)==TRUE))
  { cat("There is still missing values in the imputed dataset, please check your imputation")
    break
  }else{
    Mo<-IM$nbMI+1
    pcaM<-princomp(IM$ImpM)
    cpdimM<-as.matrix(pcaM$scores[,DIM])   
    opa<-array(cpdimM,dim=c(nrow(cpdimM),ncol(cpdimM),Mo)) 
    for(i in 1:IM$nbMI){
      pca<-princomp(IM$Mi[,,i])
      opa[,,i]<-as.matrix(pca$scores[,DIM])
    }
    if(symmetric==TRUE){
      for (i in 1:IM$nbMI+1){
        trace<-sum(opa[,,i]^2)
        opa[,,i]<-opa[,,i]/sqrt(trace) 
      }
    }
    ############################ Ordinary Procrustes Analysis (library(shapes))
    for(k in 1:IM$nbMI){
      analyse<-procOPA(opa[,,Mo],opa[,,k], reflect=TRUE)
      opa[,,k]<-analyse$Bhat
    }
    opa[,,Mo]<-analyse$Ahat
    ######################## Principal component explained variance
    pvar<-pcaM$sdev^2
    tot<-sum(pvar)
    valX<-pvar[DIM[1]]
    valY<-pvar[DIM[2]]
    valX<-round(valX*100/tot,digits=2)
    valY<-round(valY*100/tot, digits=2)
    ######################## Plot function
    op <- par(no.readonly=TRUE)
    if(scale==TRUE){
      plot(opa[,1,Mo],opa[,2,Mo], type="p", pch=3, col=c(as.factor(ifelse(complete.cases(IM$missing) ==T, 1, 5))),lwd=1,xlim=range(opa[,1,Mo]),ylim=range(opa[,1,Mo]),xlab=paste("DIM",DIM[1],valX,"%",sep=" "),ylab=paste("DIM",DIM[2],valY,"%",sep=" "))
    }
    if(scale==FALSE){
      plot(opa[,1,Mo],opa[,2,Mo], type="p", pch=3, col=c(as.factor(ifelse(complete.cases(IM$missing) ==T, 1, 5))),lwd=1,xlab=paste("DIM",DIM[1],valX,"%",sep=" "),ylab=paste("DIM",DIM[2],valY,"%",sep=" "))
    }
    title("MI effect on Multivariate Analysis", font.main=3, adj=1)
    ## Store row names
    NR<-IM$missing
    rownames(IM$missing)<-NULL
    ##
    if(ellipses==TRUE){                                      
      coul<-as.numeric(rownames(IM$missing[complete.cases(IM$missing),]))
      for (j in coul){
        lines(ELLI(opa[j,1,],opa[j,2,],np=Mo), col="black", lwd=1)}
      coul<-as.numeric(rownames(IM$missing[!complete.cases(IM$missing),]))
      for (j in coul){
        lines(ELLI(opa[j,1,],opa[j,2,],np=Mo), col="red", lwd=1)}
    }else{ points(opa[,1,],opa[,2,],cex=0.5) }
    if(web==TRUE){
      coul<-as.numeric(rownames(IM$missing[complete.cases(IM$missing),]))
      for (j in coul){ 
        for(f in 1:IM$nbMI){
          segments(opa[j,1,Mo],opa[j,2,Mo], opa[j,1,f],opa[j,2,f], col="black", lwd=1) }
      } 
      coul<-as.numeric(rownames(IM$missing[!complete.cases(IM$missing),]))
      for (j in coul){ 
        for(f in 1:IM$nbMI){
          segments(opa[j,1,Mo],opa[j,2,Mo], opa[j,1,f],opa[j,2,f], col="red", lwd=1)}
      } 
      points(opa[,1,],opa[,2,],cex=0.5) 
    } 
    nom<-rownames(NR)
    text(opa[,1,Mo],opa[,2,Mo],nom, pos=1)
    
    abline(h=0,v=0, lty=3)
    par(xpd=TRUE)  # Do not clip to the drawing area
    lambda <- .025
    legend(par("usr")[1], (1 + lambda) * par("usr")[4] - lambda * par("usr")[3],c("Complete", "Missing"), xjust = 0, yjust = 0,lwd=3, lty=1, col=c(par('fg'), 'red'))
    par(op)      
  }
}
 
# convert to dagitty and orint dag functions 
bn_to_adjmatrix <- function(bn_obj) {
  edg <- as.data.frame(bn_obj$arcs)
  node_names <- names(bn_obj$nodes)
  ans_mat <- matrix(
    data = 0, nrow = length(node_names),
    ncol = length(node_names),
    dimnames = list(node_names, node_names)
  )
  
  ans_mat[as.matrix(edg[c("from", "to")])] <- 1
  return(ans_mat)
}
###
adjmatrix_to_dagitty <- function(adjmatrix) {
  if (is.null(rownames(adjmatrix)) | is.null(colnames(adjmatrix)) | !identical(rownames(adjmatrix), colnames(adjmatrix))) {
    warning("Matrix column names or rownames are either missing or not compatible. They will be replaced by numeric node names")
    nodes <- 1:nrow(adjmatrix)
  } else {
    nodes <- rownames(adjmatrix)
  }
  
  from_to <- which(adjmatrix == 1, arr.ind = T)
  
  dag_string <- paste0(
    "dag { \n",
    paste0(nodes, collapse = "\n"),
    "\n",
    paste0(apply(from_to, 1, function(x) paste0(nodes[x[1]], " -> ", nodes[x[2]])),
           collapse = "\n"
    ),
    "\n } \n"
  )
  return(dagitty:::dagitty(dag_string))
}

##
causal_direction <- function(vec_1, vec_2, continuous_thresh, discrete_thresh) {
  if (class(vec_1) == "character") vec_1 <- factor(vec_1)
  if (class(vec_2) == "character") vec_2 <- factor(vec_2)
  y_cond_x <- function(x, y) {
    ans <- sapply(levels(x), function(x_val) {
      table(y[x == x_val]) / sum(x == x_val)
    })
    ans[is.nan(ans)] <- 0
    ans
  }
  if (class(vec_1) == "factor" & class(vec_2) == "numeric"){ 
    vec_2 <- factor(infotheo::discretize(vec_2)$X)
  }
  if (class(vec_1) == "numeric" & class(vec_2) == "factor"){ 
    vec_1 <- factor(infotheo::discretize(vec_1)$X)
  }
  if (class(vec_1) == "factor" & class(vec_2) == "factor") {
    p_vec_2_given_vec1 <- y_cond_x(x = vec_1, y = vec_2)
    p_vec_1 <- table(vec_1) / length(vec_1)
    dist_vec_1_causes_vec_2 <- energy:::dcor(p_vec_1, t(p_vec_2_given_vec1))
    
    p_vec_1_given_vec2 <- y_cond_x(x = vec_2, y = vec_1)
    p_vec_2 <- table(vec_2) / length(vec_2)
    dist_vec_2_causes_vec_1 <- energy:::dcor(p_vec_2, t(p_vec_1_given_vec2))
    
    if (dist_vec_2_causes_vec_1 - dist_vec_1_causes_vec_2 > discrete_thresh) {
      return("vec 1 causes vec 2")
    } else if(dist_vec_1_causes_vec_2 - dist_vec_2_causes_vec_1 > discrete_thresh){
      return("vec 2 causes vec 1")
    } else {
      return("not sure")
    }
  } else {
    cause_sum <- as.numeric(generalCorr:::some0Pairs(data.frame(vec_1, vec_2), verbo = F)$outVote[7])
    if (cause_sum > continuous_thresh) {
      return("vec 1 causes vec 2")
    } else if (cause_sum < -continuous_thresh){
      return("vec 2 causes vec 1")
    } else {
      return("not sure")
    }
  }
}
##
orient_dag <- function(adjmatrix, x, max_continuous_pairs_sample = 5000, continuous_thresh = 1, discrete_thresh = 0.3) {
  if (!is.matrix(adjmatrix)) {
    stop("Input DAG must be represented as adjacency matrix")
  } else {
    DAG_rownames <- rownames(adjmatrix)
    DAG_colnames <- colnames(adjmatrix)
    if (!identical(DAG_rownames, DAG_colnames)) {
      stop("DAG adjacency matrix rownames and colnames must be identical")
    }
    if (mean(DAG_rownames %in% names(x)) < 1) {
      stop("Some nodes are missing from the input data x")
    }
  }
  
  edges <- which(adjmatrix == 1, arr.ind = T)
  i = 1
  while(i <= nrow(edges)){
    dup_idx <- integer(0)
    dup_idx <- which(apply(edges[, c(2,1)], 1, function(x) x[1] == edges[i, 1] & x[2] == edges[i, 2]))
    if(length(dup_idx) == 1) {
      adjmatrix[edges[dup_idx, 1], edges[dup_idx, 2]] <- 0 # remove one of the duplicates
      edges <- edges[-dup_idx, ]
    }
    vec_1 <- x[[DAG_rownames[edges[i, 1]]]]
    vec_2 <- x[[DAG_rownames[edges[i, 2]]]]
    if (class(vec_1) == "numeric" & class(vec_2) == "numeric"){
      if(is.null(max_continuous_pairs_sample)){
        next # dont change orientation
      } else {
        samples <- sample.int(length(vec_1), min(max_continuous_pairs_sample, length(vec_1)))
        vec_1 <- vec_1[samples]
        vec_2 <- vec_2[samples]
      }
    } 
    
    cause <- causal_direction(vec_1, vec_2, 
                              continuous_thresh = ifelse(length(dup_idx) == 1, 0, continuous_thresh), # if bi-directed take whichever has higher score
                              discrete_thresh = ifelse(length(dup_idx) == 1, 0, discrete_thresh) # if bi-directed take whichever has higher score)
    )
    if (cause == "vec 1 causes vec 2") { 
      adjmatrix[edges[i, 1], edges[i, 2]] <- 1
      adjmatrix[edges[i, 2], edges[i, 1]] <- 0
    } else if(cause == "vec 2 causes vec 1"){
      adjmatrix[edges[i, 1], edges[i, 2]] <- 0
      adjmatrix[edges[i, 2], edges[i, 1]] <- 1
    }
    i <- i + 1
  }
  
  return(adjmatrix)
}
```


```{r data}
data_all_original <- read_dta("/home/rstudio/MissingdataArticle/data&Objects/Analysis_dataset(60)25.2.21.dta.dta")
data_all <- Prepare_data(data_all_original)
data_all <- Preprocess_Data(data_all)
```


# Introduction

The essential concern with missing data is the potential selection bias produced by the mechanism of the missingness @perkins2018principled. For instance, the MAR assumption, i.e. the likelihood of missing data variable Y being missing is unrelated to the value of Y after controlling for observed covariates available in the data set. A complete case model under the MAR assumption may lead to biased results because the outcome patterns in subjects with complete data may differ from those with missing data. The degree of bias can be considerable, particularly if there are large volumes of missing data @van2018flexible and @perkins2018principled. Besides, complete case analysis can also result in a significant loss of sample size @van2018flexible @perkins2018principled. 

In recent years there have been increased recommendations from scientific publications to apply methodolgical approaches to missing data problems @little2012prevention. Multiple imputations (MI) is one of the common powerful approaches to treat the incomplete data problem that has several advantages. MI works as follows. First, every missing value is being predicted based on a model that includes the other covariate in addition to some chosen auxiliary variables. Second, the process is repeated in an iterative fashion resulting in multiple completed data sets. Every time the imputation round happens, a slightly different value is produced. Under the MAR assumption, this imputation procedure produces unbiased estimates of the missing data values. (add the pooling if the pooled results are positive)

In this work, we applied the MI approach to the existing missing cases. We assessed that MI is a valid approach given that potential loss of information resulted from the missing data amount. (gives the missingness percentage to the top 3 or 5 variables). We followed the latest scientific recommendations for assessing the use of the Multiple Imputation method @nguyen2017model. In the executed MI workflow, we included all the variables that we planned to use in the Bayesian Network analysis. Besides,  We included an additional set of auxiliary variables \textcolor{red}{give some detail about those variables} that correlate or predict the pattern of the missingness (For instance, features that are related to the nonresponse or variables that are known to have influenced the occurrence of missing data such variables may contain information and  reasons for nonresponse). This is motivated by the expected improvements in the plausibility of the MAR assumption underlying MI @sterne2009multiple.  We used the mice R package to conduct the imputation in a High-Performance Computing environment @van2015package.

## Assessing the missing data

Let us start by looking at some plots to help us understand the missing data patteren better. We will be using the R package naniar for that reason.  

```{r, fig.height=5, cache=TRUE}
vis_miss(data_all, warn_large_data = FALSE)
```
```{r, echo=TRUE, cache=TRUE}
gg_miss_upset(data_all)
```
We can see from the figures above that the missingness percentage is 1.81.8. In the second figure, we see the top five variables that include missingness and the intersection between these five variables with the most amount of missingness is in the variable e_amount (electrical smoking). Let us look at some important variables for our study in more detail, In the figure below we examine the sei_class variable. We plot the percentage of the missingness within each category of the sei_class variable.

```{r, cache=TRUE}
gg_miss_fct(x = data_all, fct = sei_class)
```
We can see that people with the class 6 and 7 has a high missingness percentage compared to the rest of the classes with regards to both the trt_bp and e_amount. 

```{r, cache=TRUE}
explanatory =  c("education", "duration", "BMI")
dependent = "age"
data_all %>%  missing_pairs(dependent, explanatory)
```
We can see that people with missing values in Duration are older than those who reported that duration value. This suggests that the missing mechanism is MAR as the pattern of missingness depends on the age variable and not on duration itself. However, we still need to test for the MNAR as it may be the case that the duration itself has an effect. Let us try different variables to test against the age and education variables. For instance, the startage and syk_class variables.

```{r, cache=TRUE}
explanatory =  c("education", "startage", "syk_class")
dependent = "age"
data_all %>%  missing_pairs(dependent, explanatory)
```

The same holds for the startage variable. the same for the relationship between age and education: it seems that patients with missing education are older than with non-missing. This means that adjusting for age should solve the case.

# Missing Value Imputation

The above presentation motivates us to perform the MI using the MICE approach. The full description and reproducible workflow can be found [in this Github repository](https://github.com/ranibasna/Modification_Effect_Socioeconomic_smoking_Asthma/blob/master/R/HPC_R_mice_BN_code.R). The code is written in a style to be used inside a High-performance computing environment (HPC). The process of performing MI is going like this.

1. We first processes the original data by dropping the highly correlated variables
2. Due to high dependencies between some variables we impose which variables to include in the predictive matrix that is used by mice imputation.
3. We apply the parlmice which is the version of the R function mice that can perform a parallel computation

```{r, eval=FALSE}
parlmice(data = data_all, n.core = 8, n.imp.core = 7, maxit = 40, predictorMatrix = pred, seed = 11)
```

Where n.core is number of cores and n.imp.cores number of imputations per core.

# Validating the Imputation Process

```{r, cache=TRUE, message=FALSE}
# sample from the big imputed mice model to select smaller number of multiple imputations
imp_parl_final = readRDS(file = "/home/rstudio/socioeconomics/data&Objects/final_mice_model.rds")
datalist <- mids2datlist(imp_parl_final)
mids_subset <- subset_datlist(datalist, index = 70:85, toclass = "mids")
```

```{r, cache=TRUE}
xyplot(mids_subset,BMI ~ startage + duration,pch=18,cex=1)
```
```{r, cache=TRUE}
densityplot(mids_subset)
```

As seen the imputation model is predicting that most people with missing data are actually a smoker. This explains why the spike of non smokers has disappeared in the imputation models for the duration variable in the figure above. 

```{r, cache=TRUE}
stripplot(mids_subset, pch = 20, cex = 1.2)
```


```{r, cache=TRUE}
imp_data <- complete(imp_parl_final, 1)
```



```{r, cache=TRUE}
df_1 <- data_all %>% select(c(BMI, age)) %>% rename(BMI_imp = BMI) %>%
  mutate(BMI_imp = as.logical(ifelse(is.na(BMI_imp),"TRUE","FALSE"))) %>% rownames_to_column()
df_2 <- imp_data %>% select(age, BMI) %>% rownames_to_column()
df <- left_join(df_1,df_2)

df <- as.data.frame(df)
vars <- c("age", "BMI","BMI_imp")
marginplot(df[,vars], delimiter="imp", alpha=0.6, pch=c(19))
```

```{r, cache=TRUE}
df_1 <- data_all %>% select(c(duration, age)) %>% rename(duration_imp = duration) %>%
  mutate(duration_imp = as.logical(ifelse(is.na(duration_imp),"TRUE","FALSE"))) %>% rownames_to_column()
df_2 <- imp_data %>% select(age, duration) %>% rownames_to_column()
df <- left_join(df_1,df_2)

df <- as.data.frame(df)
vars <- c("age", "duration","duration_imp")
marginplot(df[,vars], delimiter="imp", alpha=0.6, pch=c(19))
```

We can see that the imputation model corrects for the fact that the people with older age tend to have missing values are duration and start age, which can be attributed to the fact that either they forgot the starting age (hence the duration as well). Or they did not want to include that information as it makes them feel they have been smoking for a long time. This is still valid under the MAR assumption.

```{r, cache=TRUE}
#cat_var <- sei_class
df_1 <- data_all %>% select(c(BMI, sei_class)) %>% rename(sei_class_imp = sei_class) %>%
  mutate(sei_class_imp = as.logical(ifelse(is.na(sei_class_imp),"TRUE","FALSE"))) %>% rownames_to_column()

df_2 <- imp_data %>% select(sei_class, BMI) %>% rownames_to_column()

df <- left_join(df_1,df_2)
df <- as.data.frame(df)
vars <- c("BMI","sei_class","sei_class_imp")
barMiss(df[,vars], delimiter = "_imp", selection = "any", only.miss = FALSE)
##
df_1 <- data_all %>% select(c(BMI, syk_class)) %>% rename(syk_class_imp = syk_class) %>%
  mutate(syk_class_imp = as.logical(ifelse(is.na(syk_class_imp),"TRUE","FALSE"))) %>% rownames_to_column()
df_2 <- imp_data %>% select(syk_class, BMI) %>% rownames_to_column()
df <- left_join(df_1,df_2)
df <- as.data.frame(df)
vars <- c("BMI","syk_class","syk_class_imp")
barMiss(df[,vars], delimiter = "_imp", selection = "any", only.miss = FALSE)
```

In general, all our validation plots look healthy and the imputation processes reflect our beliefs about the reason for missing values. We have been investigating only the important variables. A similar analysis process can be applied to different variables to do the checks.

# The dag 

Next we will use the dag theory to run a sensitivity analysis to validate the missing data imputation. If you are not familiar with the dag theory it is recommended to read about. For instance see @scutari2014bayesian for more details. 

(In)dependency structure can be identified in Bayesian networks, particularly with Directed Acyclic Graphs (DAGs), which are probabilistic graphical models. DAGs learn the underlying dependency structure represent these as networks with directed connections. In simple, the dag allow us to find a minimal set of variables that is needed to study the effect of specific exposure and on an outcome. 

```{r BNObject}
str.bootstrap_mice_hc <- readRDS(file = "/home/rstudio/MissingdataArticle/data&Objects/boot_mice_final_hc.rds")
str.bootstrap_mice_hc_filterd <- str.bootstrap_mice_hc[with(str.bootstrap_mice_hc, strength >= 0.95 & direction >= 0.5), ]
avg.simpler_mice_hc  <- averaged.network(str.bootstrap_mice_hc_filterd)
avg.simpler_mice_hc  <- cextend(averaged.network(str.bootstrap_mice_hc_filterd))
# convert to dagitty structure
bn_dagitty <- bn_to_adjmatrix(cextend(avg.simpler_mice_hc))
bn_dagitty <- adjmatrix_to_dagitty(bn_dagitty)
```

```{r}
# install.packages("ggdag")
# library(ggdag)
bn_dag_adjustment <- adjustmentSets(x = bn_dagitty, exposure = c("sei_class"), outcome = c("c_asthma"), effect = "direct")
adjustment_vars <- c(bn_dag_adjustment[[2]], c("education")) 
```


# Sensitivity analysis

Since the MAR hypothesis cannot be verified from observed data @van2018flexible, it is essential to perform sensitivity analyses. Sensitivity analyses will appraise the impact on the current research question imputation results of departures from this MAR assumption.

One proposed method is the use of the delta adjustment method. This approach allows a simple and flexible way by which to impute the incomplete data under general MNAR mechanisms, and thus to assess sensitivity to departures from MAR. The method was originally introduced first by the work of Rubin @rubin1977formalizing, it has also been described and used by van Buuren et al. @van2018flexible and implemented for a variety of variable types in the R package SensMice by Resseguier et al @resseguier2011sensitivity. which we used in our work.


To do sensitivity analysis, we apply multiple forms of perturbations to the variables with missing data before applying the imputation process. Then we conduct a regression analysis on the different perturbed imputed sets. The hypothesis is that if the missingness mechanism is of MAR type, then the results should be similar as the imputation process uses the predictive covariates to predict the missing values and the missing value pattern is independent of the value of the variable itself.  

In Specific, After fitting an imputation model for the incomplete variable Y under MAR, implementation of the delta-adjustment method involves adding or deducting a fixed number $\delta$ to the current predictor before imputing missing data using the proposed model. In this sense, it is a simple representation of the pattern-mixture model. When Y is binary and the incomplete data are predicted using a logistic regression model, $\delta$ represents the difference in the log-odds of Y = 1 for individuals with missing Y values compared with individuals with observed Y values.


We follow the 3-step strategy proposed by @resseguier2011sensitivity

 1. Fit an imputation model assuming ignorable missing Data;
 2. Adjust the imputation model by adding a fixed quantity $\delta$  to the linear predictor before imputing missing data using the updated model. This $\delta$ specifies a realistic scenario ( usually this is a set of plausible $\delta$-values that assume a MNAR situation). When the variable with missing data Y, is binary and the missing data are imputed using a logistic regression model, $\delta$ represents the difference in the log-odds of Y for individuals with missing Y values compared with individuals with observed Y values.
 3. Impute missing Data under the scenario specified in point 2.

In practice, we will select test the above explained approach on the syk_class valriable. The same can be done on other important variable. syk_class is chosen since it has plenty of missingness compared to other variable as well as since it is one of the key variable in the study. For this analysis to be preformed, we will use three $\delta$ values $0,1$ and $1.5$.    

```{r SenAna, cache=TRUE}
# e_amount, sei_class and syk_class has the highst missingness. 
imp_test_method <- mids_subset
# changing the type to polyreg since polr is not yet supported in the package SensMice
imp_test_method$method["sei_class"] <- "polyreg"
imp_test_method$method["syk_class"] <- "polyreg"
# get a mice object with the sensitivity analysis
#
sen_ana_mice_imp_1 <- sens.mice(IM = imp_test_method, ListMethod = c("","","","","","","","","","","","","","","","","","","","","","","","","","","","","","", "","","MyFunc","","",""), SupPar = c(0.5, 0.5,0.5,0.5,0.5,0.5, 0.5, 0.5, 0.5,0.5))
#
sen_ana_mice_imp_2 <- sens.mice(IM = imp_test_method, ListMethod = c("","","","","","","","","","","","","","","","","","","","","","","","","","","","","","", "","","MyFunc","","",""), SupPar = c(1, 1,1,1,1,1, 1, 1, 1,1))
#
sen_ana_mice_imp_3 <- sens.mice(IM = imp_test_method, ListMethod = c("","","","","","","","","","","","","","","","","","","","","","","","","","","","","","", "","","MyFunc","","",""), SupPar = c(1.5, 1.5,1.5,1.5,1.5,1.5, 1.5, 1.5, 1.5,1.5))
```

```{r Senfitting, cache=TRUE}
# fit a glm model on the main mice object
fit_c_as_main <- with(mids_subset, glm(as.formula(paste("c_asthma ~ ", paste(adjustment_vars, collapse= "+"))), family = binomial))
fits_pool <- fit_c_as_main %>% mice::pool()

# fit the sen_ana sensitivity analysis imputed data
fit_c_as_sen_ana_big <- with(sen_ana_mice_imp_1, glm(as.formula(paste("c_asthma ~ ", paste(adjustment_vars, collapse= "+"))), family = binomial))
fits_pool_sen <- fit_c_as_sen_ana_big %>% mice::pool()

# 
fit_c_as_sen_ana_big_2 <- with(sen_ana_mice_imp_2, glm(as.formula(paste("c_asthma ~ ", paste(adjustment_vars, collapse= "+"))), family = binomial))
fits_pool_sen_2 <- fit_c_as_sen_ana_big_2 %>% mice::pool()

#
fit_c_as_sen_ana_big_3 <- with(sen_ana_mice_imp_3, glm(as.formula(paste("c_asthma ~ ", paste(adjustment_vars, collapse= "+"))), family = binomial))
fits_pool_sen_3 <- fit_c_as_sen_ana_big_3 %>% mice::pool()
```

Let us looks at the summary results of the original regressing model and the model that uses $\delta$ value equal $1$.



::: {.l-page}
::: {.panelset}

::: {.panel}
## 👋 Original Model! {.panel-name}


```{r}
summary(fits_pool, conf.int=TRUE, exponentiate = TRUE)
```
:::

::: {.panel}
## model that uses $\delta$ value equal $0.5$. {.panel-name}

```{r}
summary(fits_pool_sen, conf.int=TRUE, exponentiate = TRUE)
```
:::

::: {.panel}
## model that uses $\delta$ value equal $1$. {.panel-name}

```{r}
summary(fits_pool_sen_2, conf.int=TRUE, exponentiate = TRUE)
```
:::

::: {.panel}
## model that uses $\delta$ value equal $1.5$. {.panel-name}

```{r}
summary(fits_pool_sen_3, conf.int=TRUE, exponentiate = TRUE)
```
:::

:::
:::

An esier way to compare is to plot the odds ratios for all the four models

::: {.l-page}
::: {.panelset}

::: {.panel}
## 👋 Original Model! {.panel-name}


```{r}
explanatory <- adjustment_vars
dependent <- "c_asthma"
suppressWarnings(mids_subset$data %>% or_plot(dependent, explanatory, glmfit = fits_pool, table_text_size=4, confint_type = "profile"))
```
:::

::: {.panel}
## model that uses $\delta$ value equal $0.5$. {.panel-name}

```{r}
suppressWarnings(sen_ana_mice_imp_1$data %>% or_plot(dependent, explanatory, glmfit = fits_pool_sen, table_text_size=4, confint_type = "profile"))
```
:::

::: {.panel}
## model that uses $\delta$ value equal $1$. {.panel-name}

```{r}
suppressWarnings(sen_ana_mice_imp_2$data %>% or_plot(dependent, explanatory, glmfit = fits_pool_sen_2, table_text_size=4, confint_type = "profile"))
```
:::

::: {.panel}
## model that uses $\delta$ value equal $1.5$. {.panel-name}

```{r}
suppressWarnings(sen_ana_mice_imp_3$data %>% or_plot(dependent, explanatory, glmfit = fits_pool_sen_3, table_text_size=4, confint_type = "profile"))
```
:::

:::
:::


<!-- ```{r OR_1, cache=TRUE, out.width = "50%", fig.show = "hold", fig.align = "default"} -->
<!-- explanatory <- adjustment_vars -->
<!-- dependent <- "c_asthma" -->

<!-- # par(mar = c(4, 4, .1, .1)) -->

<!-- # imp_test_method$data %>% or_plot(dependent, explanatory, glmfit = fits_pool, table_text_size=4) -->
<!-- # mids_subset$data %>% or_plot(dependent, explanatory, glmfit = fits_pool, table_text_size=4) -->
<!-- # suppressWarnings(mids_subset$data %>% or_plot(dependent, explanatory, glmfit = fits_pool, table_text_size=4)) -->
<!-- # data_modeling %>% or_plot(dependent, explanatory, glmfit = fits_pool, table_text_size=4) -->

<!-- # the sens model -->
<!-- suppressWarnings(sen_ana_mice_imp_2$data %>% or_plot(dependent, explanatory, glmfit = fits_pool_sen_2, table_text_size=4, confint_type = "profile")) -->
<!-- suppressWarnings(sen_ana_mice_imp_3$data %>% or_plot(dependent, explanatory, glmfit = fits_pool_sen_3, table_text_size=4, confint_type = "profile")) -->
<!-- ``` -->




<!-- ```{r, echo=FALSE, include=FALSE} -->
<!-- images = c( -->
<!--   "./OR_2.png", -->
<!--   "./OR_3.png" -->
<!-- ) -->
<!-- ``` -->

<!-- ```{r ORPlot,echo=FALSE, out.width = "50%", fig.show = "hold", fig.align = "default"} -->
<!-- # par(mar=c(4,4,0.1,0.1)) -->
<!-- knitr::include_graphics(images[1]) -->
<!-- knitr::include_graphics(images[2]) -->
<!-- # knitr::include_graphics(images[3]) -->
<!-- ``` -->


As seen from the above plot the fours models results in similar odss ratios.  

## Conclusion
A sensitivity analysis was performed on data from the WSAS and OLIN, to assess the robustness of the OR between asthma and education, including missing Data. We assumed that non-responders were more likely to have a lower education compared to the responders. This assumption has been reflected with the inserted set of $\delta$ values. The modified ORs were stable and robust to the multiple introduced perturbation and scenarios. This strengthens our approach of using the multiple imputations assuming the MAR mechanism. 


# Reference